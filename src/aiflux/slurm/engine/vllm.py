from pathlib import Path

def create_vllm_batch_script(
        account: str,
        partition: str,
        nodes: str,
        gpus_per_node: str,
        time: str,
        memory: str,
        cpus_per_task: str,
        logs_dir: Path,
        input_file: Path,
        output_file: Path,
):
    # Create SLURM job script
    job_script = [
        "#!/bin/bash",
        f"#SBATCH --job-name=llm_processor",
        f"#SBATCH --account={account}",
        f"#SBATCH --partition={partition}",
        f"#SBATCH --nodes={nodes}",
        f"#SBATCH --gpus-per-node={gpus_per_node}",
        f"#SBATCH --time={time}",
        f"#SBATCH --mem={memory}",
        f"#SBATCH --cpus-per-task={cpus_per_task}",
        f"#SBATCH --output={logs_dir}/%j.out",
        f"#SBATCH --error={logs_dir}/%j.err",
        "",
        "# Load required modules",
        "module purge",
        "",
        "# Try loading GCC",
        "for gcc_version in '11.4.0' '11.3.0'; do",
        "    if module load gcc/$gcc_version &>/dev/null; then",
        "        echo \"Loaded gcc/$gcc_version\"",
        "        break",
        "    fi",
        "done",
        "",
        "# Try loading CUDA",
        "for cuda_version in '12.2.1' '11.7.0'; do",
        "    if module load cuda/$cuda_version &>/dev/null; then",
        "        echo \"Loaded cuda/$cuda_version\"",
        "        break",
        "    fi",
        "done",
        "",
        "# Create all necessary directories",
        "mkdir -p $DATA_INPUT_DIR $DATA_OUTPUT_DIR $MODELS_DIR $LOGS_DIR $CONTAINERS_DIR $APPTAINER_TMPDIR $APPTAINER_CACHEDIR",
        "",
        "# Start Ollama server",
        "mkdir -p $VLLM_HOME $VLLM_MODELS",
        "",
        "# Build container if needed",
        "if [ ! -f \"$CONTAINERS_DIR/llm_processor.sif\" ]; then",
        "    APPTAINER_DEBUG=1 apptainer build --force $CONTAINERS_DIR/llm_processor.sif $CONTAINER_DEF",
        "fi",
        "",
        "# Start server",
        "VLLM_DEBUG=1 apptainer exec --nv \\",
        "    --env VLLM_HOST=$VLLM_HOST \\",
        "    --env VLLM_ORIGINS=* \\",
        "    --env VLLM_MODELS=$VLLM_MODELS \\",
        "    --env VLLM_HOME=$VLLM_HOME \\",
        "    --env VLLM_INSECURE=true \\",
        "    --env CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES \\",
        "    --env VLLM_SCHED_SPREAD=$VLLM_SCHED_SPREAD \\",
        "    --env CURL_CA_BUNDLE= \\",
        "    --env SSL_CERT_FILE= \\",
        "    --bind $DATA_INPUT_DIR:/app/data/input,$DATA_OUTPUT_DIR:/app/data/output,$MODELS_DIR:/app/models,$LOGS_DIR:/app/logs,$VLLM_HOME:$VLLM_HOME \\",
        "    $CONTAINERS_DIR/llm_processor.sif \\",
        "    vllm serve &",
        "",
        "VLLM_PID=$!",
        "",
        "# Wait for server",
        "for i in {1..60}; do",
        "    if curl -s \"http://localhost:$VLLM_PORT/api/version\" &>/dev/null; then",
        "        echo \"Ollama server started\"",
        "        break",
        "    fi",
        "    if ! ps -p $VLLM_PID > /dev/null; then",
        "        echo \"Ollama server died\"",
        "        exit 1",
        "    fi",
        "    echo \"Waiting... ($i/60)\"",
        "    sleep 1",
        "done",
        "",
        "# Pull model if needed",
        "MODEL_NAME=\"${VLLM_MODEL_NAME:-llama3.2:3b}\"",
        "echo \"Checking if model ${MODEL_NAME} exists...\"",
        "",
        # "# Extract base model name for Ollama (e.g. llama3.2:3b -> llama3.2)",
        # "if [[ \"$MODEL_NAME\" == *\":\"* ]]; then",
        # "    BASE_MODEL=$(echo \"$MODEL_NAME\" | cut -d':' -f1)",
        # "    echo \"Using base model name for Ollama: $BASE_MODEL\"",
        # "else",
        # "    BASE_MODEL=\"$MODEL_NAME\"",
        # "fi",
        "",
        "# Check if model exists, try to pull if it doesn't",
        "if ! curl -s \"http://localhost:$VLLM_PORT/api/tags\" | grep -q \"\\\"name\\\":\\\"$MODEL_NAME\\\"\"; then",
        "    echo \"Model not found, pulling base model ${MODEL_NAME}...\"",
        "    curl -X POST \"http://localhost:$VLLM_PORT/api/pull\" -d '{\"name\": \"'\"$MODEL_NAME\"'\"}' -H \"Content-Type: application/json\"",
        "    if [ $? -ne 0 ]; then",
        "        echo \"Failed to pull model ${MODEL_NAME}\"",
        "        echo \"Available models:\"",
        "        curl -s \"http://localhost:$VLLM_PORT/api/tags\" | grep -o '\"name\":\"[^\"]*\"' || echo \"None found\"",
        "        exit 1",
        "    else",
        "        echo \"Successfully pulled model ${MODEL_NAME}\"",
        "    fi",
        "else",
        "    echo \"Model ${MODEL_NAME} already exists\"",
        "fi",
        "",
        "# Run processor",
        f"python3 -c \"",
        "import sys",
        "import os",
        "sys.path.append('$PROJECT_ROOT')",
        "from aiflux.core.config import Config",
        "from aiflux.processors import BatchProcessor",
        "",
        "# Ensure VLLM environment variables are available in Python",
        "vllm_port = os.environ.get('VLLM_PORT')",
        "if vllm_port:",
        "    os.environ['VLLM_HOST'] = f'http://localhost:{vllm_port}'",
        "",
        "# Load model configuration",
        "config = Config()",
        "model_name = os.environ.get('MODEL_NAME', 'llama3.2:3b')",
        "model_type = model_name.split(':')[0] if ':' in model_name else model_name",
        "model_size = model_name.split(':')[1] if ':' in model_name else '3b'",
        "",
        "try:",
        "    model_config = config.load_model_config(model_type, model_size)",
        "except Exception as e:",
        "    print(f'Error loading model config for {model_type}:{model_size}: {e}')",
        "    # Fallback to default model",
        "    model_config = config.load_model_config('qwen2.5', '7b')",
        "",
        "# Create batch processor with JSONL input",
        "batch_processor = BatchProcessor(",
        "    model_config=model_config,",
        "    batch_size=int(os.environ.get('BATCH_SIZE', '4')),",
        "    save_frequency=int(os.environ.get('SAVE_FREQUENCY', '50')),",
        "    max_retries=int(os.environ.get('MAX_RETRIES', '3')),",
        "    retry_delay=float(os.environ.get('RETRY_DELAY', '1.0'))",
        ")",
        "",
        "# Prepare run kwargs",
        "run_kwargs = {}",
        "",
        "# Add any other kwargs from environment variables",
        "for key in ['max_tokens', 'temperature', 'top_p', 'top_k']:",
        "    if key.upper() in os.environ:",
        "        run_kwargs[key] = os.environ[key.upper()]",
        "",
        f"batch_processor.run('{input_file}', '{output_file}', **run_kwargs)",
        "\"",
        "",
        "# Cleanup",
        "pkill -f \"vllm serve\" || true",
        "# Only remove temporary directories that we created",
        "if [ -d \"$APPTAINER_TMPDIR\" ] && [ -w \"$APPTAINER_TMPDIR\" ]; then",
        "    rm -rf \"$APPTAINER_TMPDIR\"",
        "fi",
        "if [ -d \"$APPTAINER_CACHEDIR\" ] && [ -w \"$APPTAINER_CACHEDIR\" ]; then",
        "    rm -rf \"$APPTAINER_CACHEDIR\"",
        "fi"
    ]
    return job_script