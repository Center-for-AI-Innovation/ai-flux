# Gemma 3 4B Configuration (Vision-capable model)
name: "gemma3:4b"

resources:
  gpu_layers: 32
  gpu_memory: "8GB"
  batch_size: 8
  max_concurrent: 2

parameters:
  temperature: 0.7
  top_p: 0.9
  max_tokens: 2048
  stop_sequences: ["<end_of_turn>"]  # Default stop sequence for Gemma 3

system:
  prompt: "You are a helpful AI assistant. You can understand both text and images provided by the user. Answer accurately and helpfully based on the visual and textual information."

validation:
  temperature_range: [0.0, 1.0]
  max_tokens_limit: 4096
  batch_size_range: [1, 16]
  concurrent_range: [1, 4]

# Resource Requirements
requirements:
  min_gpu_memory: "8GB"  # Minimum required for safe operation
  recommended_gpu: "RTX 3080"
  cuda_version: ">=11.7"
  cpu_threads: 4
  gpu_memory_utilization: 0.9 