Bootstrap: docker
From: ollama/ollama:latest

%post
    # Prevent interactive prompts
    export DEBIAN_FRONTEND=noninteractive
    export TZ=UTC

    # System updates and dependencies
    apt-get -y update
    apt-get -y install apt-utils
    apt-get -y install \
        python3 \
        python3-pip \
        git \
        nvidia-cuda-toolkit \
        nvidia-cuda-toolkit-gcc \
        curl \
        netcat \
        htop \
        nvidia-utils-525 \
        openssh-client

    # Create application structure with proper permissions
    mkdir -p /app/{models,data,logs}
    chmod -R 777 /app
    mkdir -p /var/log/ollama
    chmod -R 777 /var/log/ollama
    
    # Install Python dependencies
    pip3 install --no-cache-dir \
        torch==2.1.0 \
        numpy==1.24.0 \
        pandas==2.0.3 \
        pyyaml==6.0.1 \
        requests==2.31.0 \
        tqdm==4.66.1 \
        psutil

%environment
    export LC_ALL=C
    export PATH=/usr/local/cuda/bin:$PATH
    export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
    # NVIDIA driver configuration
    export NVIDIA_VISIBLE_DEVICES=all
    export NVIDIA_DRIVER_CAPABILITIES=compute,utility
    export NVIDIA_REQUIRE_CUDA="cuda>=12.0"
    export CUDA_VISIBLE_DEVICES=0
    # Ollama optimization settings - using environment variables
    export OLLAMA_GPU_LAYERS=${GPU_LAYERS:-35}
    export OLLAMA_COMMIT_INTERVAL=${OLLAMA_COMMIT_INTERVAL:-100}
    export OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE:-5m}
    export OLLAMA_ORIGINS=${OLLAMA_ORIGINS:-"*"}

%runscript
    # Start Ollama server with provided configuration
    exec ollama serve

%labels
    Author Rohan Marwaha
    Version v1.0
    Description LLM Batch Processing Container

%startscript
    # Ensure proper permissions for runtime directories
    mkdir -p "${OLLAMA_HOME:-/root/.ollama}"
    chmod -R 777 "${OLLAMA_HOME:-/root/.ollama}"
    exec ollama serve 