Bootstrap: docker
From: ollama/ollama:latest

%post
    # Prevent interactive prompts
    export DEBIAN_FRONTEND=noninteractive
    export TZ=UTC
    echo "Checking OS release"
    cat /etc/os-release || true
    echo "Installing system dependencies (Ubuntu/Debian)"
    # ollama/ollama base image is Ubuntu-based and includes CUDA runtime
    apt-get -y update
    apt-get -y install \
        python3 \
        python3-pip \
        python3-venv \
        git \
        curl \
        netcat-traditional \
        htop \
        openssh-client

    # Create application structure with proper permissions
    mkdir -p /app/{models,data,logs}
    chmod -R 777 /app
    mkdir -p /var/log/ollama
    chmod -R 777 /var/log/ollama

    # Create virtual environment
    python3 -m venv /app/venv
    # POSIX-compliant activation under /bin/sh
    . /app/venv/bin/activate

    echo "Checking NVIDIA GPU"
    # nvidia-smi
    
    # Upgrade pip and install Python dependencies compatible with CUDA 12.x and Ollama
    python -m pip install --upgrade pip

    # Core Python dependencies (latest versions)
    pip install --no-cache-dir \
        numpy \
        pandas \
        pyyaml \
        requests \
        tqdm \
        psutil

    # Install PyTorch with latest CUDA 12.9 wheels
    pip install --no-cache-dir --index-url https://download.pytorch.org/whl/cu129 \
        torch \
        torchvision \
        torchaudio
    
    # Install vLLM for high-performance LLM inference
    pip install --no-cache-dir vllm

%environment
    export LC_ALL=C
    export PATH=/app/venv/bin:/usr/local/cuda/bin:$PATH
    export VIRTUAL_ENV=/app/venv
    export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
    # NVIDIA driver configuration
    export NVIDIA_VISIBLE_DEVICES=all
    export NVIDIA_DRIVER_CAPABILITIES=compute,utility
    export NVIDIA_REQUIRE_CUDA="cuda>=12.0"
    export CUDA_VISIBLE_DEVICES=0
    # Ollama optimization settings - using environment variables
    export OLLAMA_GPU_LAYERS=${GPU_LAYERS:-35}
    export OLLAMA_COMMIT_INTERVAL=${OLLAMA_COMMIT_INTERVAL:-100}
    export OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE:-5m}
    export OLLAMA_ORIGINS=${OLLAMA_ORIGINS:-"*"}

%runscript
    # Start Ollama server with provided configuration
    exec ollama serve

%labels
    Author Rohan Marwaha, Darren Adams
    Version v1.1
    Description LLM Batch Processing Container
    LastModified 2025-09-25
    ModifiedBy Darren Adams

%startscript
    # Ensure proper permissions for runtime directories
    mkdir -p "${OLLAMA_HOME:-/root/.ollama}"
    chmod -R 777 "${OLLAMA_HOME:-/root/.ollama}"
    # Activate venv for runtime shell
    . /app/venv/bin/activate || true
    exec ollama serve 