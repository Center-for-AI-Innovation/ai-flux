Bootstrap: docker
From: ollama/ollama:latest

%post
    # Prevent interactive prompts
    export DEBIAN_FRONTEND=noninteractive
    export TZ=UTC
    echo "Checking OS release"
    cat /etc/os-release || true
    echo "Installing system dependencies (Ubuntu/Debian)"
    # ollama/ollama base image is Ubuntu-based and includes CUDA runtime
    apt-get -y update
    apt-get -y install \
        python3 \
        python3-pip \
        python3-venv \
        git \
        curl \
        netcat-traditional \
        htop \
        openssh-client

    # Create application structure with proper permissions
    mkdir -p /app/{models,data,logs}
    chmod -R 777 /app
    mkdir -p /var/log/vllm
    chmod -R 777 /var/log/vllm

    python -m pip install --upgrade pip

    # Install vLLM for high-performance LLM inference
    pip install uv
    uv venv --python 3.12 --seed vllmenv
    source .vllmenv/bin/activate
    uv pip install --no-cache-dir vllm --torch-backend=auto

%environment
    export LC_ALL=C
    export PATH=/app/venv/bin:/usr/local/cuda/bin:$PATH
    export VIRTUAL_ENV=/app/venv
    export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
    # NVIDIA driver configuration
    export NVIDIA_VISIBLE_DEVICES=all
    export NVIDIA_DRIVER_CAPABILITIES=compute,utility
    export NVIDIA_REQUIRE_CUDA="cuda>=12.0"
    export CUDA_VISIBLE_DEVICES=0
    # Ollama optimization settings - using environment variables
    export OLLAMA_GPU_LAYERS=${GPU_LAYERS:-35}
    export OLLAMA_COMMIT_INTERVAL=${OLLAMA_COMMIT_INTERVAL:-100}
    export OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE:-5m}
    export OLLAMA_ORIGINS=${OLLAMA_ORIGINS:-"*"}

%runscript
    # Start VLLM server with provided configuration
    exec vllm serve

%labels
    Author Rohan Marwaha, Darren Adams, Joshua Allen
    Version v1.1
    Description LLM Batch Processing Container
    LastModified 2025-10-20
    ModifiedBy Joshua Allen

%startscript
    # Ensure proper permissions for runtime directories
    mkdir -p "${VLLM_HOME:-/root/.vllm}"
    chmod -R 777 "${VLLM_HOME:-/root/.vllm}"
    # Activate venv for runtime shell
    . /app/vllmenv/bin/activate || true
    exec vllm serve